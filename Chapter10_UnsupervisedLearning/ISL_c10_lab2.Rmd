---
title: "ISL Chapter 10 -- Unsupervised Learning - Lab 2: Clustering"
output: html_notebook
---

# K-Means Clustering

The function kmeans() performs K-meeans clustering in R. We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 obs have a mean shift relative to the next 25 obs.
```{r}
set.seed(2)
x <- matrix(rnorm(50*2), ncol = 2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
# we now perform K-means clustering with K = 2
km_out <- kmeans(x , 2, nstart = 20)
# The cluster assignments of the 50 observations are contained in
km_out$cluster
```
The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kemeans(). We can plot the data, with each observation colored according to its cluster assignment.
```{r}
plot(x, col = (km_out$cluster + 1), main = "K-Means Clustering Results with K = 2", xlab = "", ylab = "", pch = 20, cex = 2)
```
Here the obs can be easily plotted because they are two-dimensional. 
If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors. In this example, we knew that there  really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters.

We could instead have performed K-means clustering on this example with K = 3.
```{r}
set.seed(4)
km_out <- kmeans(x, 3, nstart = 20)
km_out
```
When K = 3, K-means splits up the two clusters. To run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument. If a value of nstart greater than one is used, then K-means clustering will be performed using multiple random assignments, and the kmeans() function will report only the best results. Here we compare nstart = 1 to nstart = 20
```{r}
set.seed(3)
km_out <- kmeans(x, 3, nstart = 1)
km_out$tot.withins
km_out <- kmeans(x, 3, nstart = 20)
km_out$tot.withins
```
Note that km_out$tot.withins is the total within-cluster sum of squares, which we seek to minimize by performing K-means clustering.

The individual within-cluster sum-of-squares are contained in the vector km_out$withins

We strongly recommend always running K-means clustering with a large value of nstart, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.

When perfroming K-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed, This way, the initial cluster assignments can be replicated, and the K-means output will be fully reproducible.

# Hierarchical Clustering

The hclust() function implements hierarchical clustering in R. In the following example we use the same data used in the previous section to plot the thierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidian distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The dist() function is used to compute the 50 x 50 inter-observation Eculidian distance matrix
```{r}
hc_complete <- hclust(dist(x), method = "complete") 
# We could just as easily perform hierarchical clustering with average of single linkage instead:
hc_average <- hclust(dist(x), method = "average")
hc_single <- hclust(dist(x), method = "single")
```
We can now plot the dengrograms obtained using the usual plot() fuynction. The numbers at the bottom of the plot identify each observation.
```{r}
par(mfrow = c(1,3)) # let's plot 3 subplots 
plot(hc_complete, main = "Complete linkage", xlab = "", ylab = "", sub = "", cex = .9)
plot(hc_average, main = "Average linkage", xlab = "", ylab = "", sub = "", cex = .9)
plot(hc_single, main = "Single linkage", xlab = "", ylab = "", sub = "", cex = .9)
```
To determine the cluster labels for each observation associated with a given guct of the dendrogram, we can use the cutree() function:
```{r}
cutree(hc_complete,2)
cutree(hc_average,2)
cutree(hc_single,2)
```
For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point as belonging to its own cluster. A more senisble answer is obtained when four cluster are selected, although there are still two singletons.
```{r}
cutree(hc_single, 4) 
```
To scale the variables before performing hierarchical clustering of the observations, we use the scale() function:
```{r}
xsc <- scale(x) # by default it normalized the data
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features")
```
Correlation-based distance can be computed using the as.dist() function, which converts an arbitrary square symmetric matrix into a form that the hclust() function recorgnizes as a distance matrix. 

However, this only makes sense for data with at least 3 features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three dimensional dataset. 
```{r}
x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")
```