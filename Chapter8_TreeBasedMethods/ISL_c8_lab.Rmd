---
title: "ISL Chapter 8 -- Tree-Based Methods"
output: html_notebook
---


# ISL Chapter 8 -- Tree-Based Methods

# Fitting Classification Trees

The tree library is used to construct classification and regression trees

We first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable. We use the ifelse() function to create a variable, called High, which takes on a value of Yes if the Sales varaible exceeds 8, and takes on a value of No otherwise.

```{r}
library(tree)
library(ISLR)
attach(Carseats)
High <- ifelse(Sales <= 8, "No",  "Yes")
```
Finally, we use the data.frame() function to merge High with the rest of the Carseats data

Then we use the tree() function to fit a classification tree in order to predict High using all variables but Sales. The syntax of the tree() function is quite similar to that of the lm() function
```{r}
Carseats_df <- data.frame(Carseats, High)
tree_carseats <- tree(High~. -Sales, Carseats_df)
```
The summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.
```{r}
summary(tree_carseats)
{plot(tree_carseats)
text(tree_carseats, pretty = 0)
}
```
We see that the training error rate is 9%. For classification trees the deviance reported in the output of summary() is given by (see pg. 325). A small deviance indicates a tree that provides a good fit to the (training) data.
The residual mean deviance reported is simply the deviance divided by n - |T0|. |T0| is the number of terminal nodes on the first large tree (before pruning).

One of the most attractive properties of trees is that they can be graphically displayed. We use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty=0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.

The most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.

If we just type the name of the tree object, R prints output corresponding to each branch of the tree. R displays the split criterion (e.g. Price<92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asterisks.

```{r}
tree_carseats
```
In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error.
We split the obs into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data.
The predict() function can be used for this purpose. In the case of the classification tree, the argument type = "class" instructs R to return the actual class prediction. This approach leads to correct predictions for around 71.5% of the locations in the test data set.
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats_df), 200)
Carseats_test <- Carseats_df[-train,]
High_test <- High[-train]
tree_carseats <- tree(High ~. -Sales, Carseats_df, subset = train)
tree_pred <- predict(tree_carseats, Carseats_test, type = "class")
cm <- table(tree_pred, High_test) # confusion matrix
cm
(cm[1,1] + cm[2,2]) / sum(cm) # correct prediction rate
```
Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classifiction error rate to guide the cross-validation and pruning proccess, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to alpha in chapter 8 of the book).
```{r}
set.seed(3)
cv_carseats <- cv.tree(tree_carseats, FUN = prune.misclass)
names(cv_carseats)
cv_carseats
```
Note that, despite the name, dev corresponds to the cross-validation error rate in this instance. The tree with 9 terminal nodes results in the lowest cross-validation error rate, with 50 cross-validation errors. We plot the error rate as a function of both size and k (alpha).
```{r}
par(mfrow = c(1,2))
plot(cv_carseats$size, cv_carseats$dev, type = "b")
plot(cv_carseats$k, cv_carseats$dev, type = "b")
```
We now apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree
```{r}
prune_carseats <- prune.misclass(tree_carseats, best = 9)
{plot(prune_carseats)
text(prune_carseats, pretty = 0)
}

```
How does this pruned tree perform on the test data set? Once again, we apply the predict() function.
```{r}
tree_pred <- predict(prune_carseats, Carseats_test, type = "class")
cm <- table(tree_pred, High_test) # confusion matrix
cm
(cm[1,1] + cm[2,2]) / sum(cm) # correct prediction rate
```
Now 77% of the test obs are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy.

If we increase the value of best, we obtain a larger pruned tree with lower classification accuracy:
```{r}
prune_carseats <- prune.misclass(tree_carseats, best = 15)
{plot(prune_carseats)
text(prune_carseats, pretty = 0)  
}
tree_pred <- predict(prune_carseats, Carseats_test, type = "class")
cm <- table(tree_pred, High_test) # confusion matrix
cm
(cm[1,1] + cm[2,2]) / sum(cm) # correct prediction rate
```


# Fitting Regression Trees

Here we fit a regression treee to the Boston data set. First, we create a training set, and fit the tree to the training data. The response medv, the median value of owner-occupied homes in \$1000s.

```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2) # use half of the dataset fro training
tree_boston <- tree(medv ~., Boston, subset = train)
summary(tree_boston)
```
Notice that the output of summary() indicates that only three of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.
```{r}
{plot(tree_boston)
text(tree_boston, pretty = 0)
}
```
The variable lstat measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of lstat correspond to more epensive houses. The tree predicts a median house price of 46.4k dollars for larger homes (rm >= 7.437) in suburbs in which residents have high socioeconomic status (lstat < 9.715).

Now we use the cv.tree() function to see whether pruning the tree will improve performance
```{r}
cv_boston <- cv.tree(tree_boston)
plot(cv_boston$size, cv_boston$dev, type = "b")
```
In this case, the most complex tree is selected by cross-validation. However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:
```{r}
prune_boston <- prune.tree(tree_boston, best = 5)
{plot(prune_boston)
text(prune_boston, pretty = 0)
}

```
In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.
```{r}
yhat <- predict(tree_boston, newdata = Boston[-train,]) # predicted response
boston_test <- Boston[-train, "medv"] # observed response
plot(yhat, boston_test)
abline(0,1) 
mean((yhat - boston_test)^2)
```
In other words, the test set MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005, indicating that this model leads to test predictions that are within around 5.005k dollars of the true median home value for the suburb.

# Bagging and Random Forests

Here we apply bagging and random forests to the Boston data, using the randomForest package in R. The exact results obtained in this section may depend on the version of R and the version of the randomForest package installed on your computer. Recall that bagging is simply a special case of a random forests with m = p (number of predictors used for each split).

Therefore, the randomForest() function can be used to perform both random forests and bagging. We perform bagging as follows:

```{r}
library(randomForest)
set.seed(1)
bag_boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE)
bag_boston
```
the argument mtry = 13 indicates that all 13 predictors should be considered for each split of the tree (m = p) -- in other words, that bagging should be done. How well does this bagged model perform on the test set?
```{r}
yhat_bag <- predict(bag_boston, newdata = Boston[-train,])
plot(yhat_bag, boston_test)
abline(0,1)
mean((yhat_bag - boston_test)^2)
```
The test set MSE associaed with the baged regression tree is 13.16, almost half that obtained using an optimally-pruned single tree. We could change the number of trees grown by randomForest() using the ntree argument:
we tell the function to grow 25 trees (default is 500)
```{r}
bag_boston <- randomForest(medv~ ., data = Boston, subset = train, mtry = 13, ntree = 25)
yhat_bag <- predict(bag_boston, newdata = Boston[-train,])
bag_boston
mean((yhat_bag - boston_test)^2)
```
We see that the performance is pretty smilar

Growing a random forests proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry = 6.
```{r}
set.seed(1)
rf_boston <- randomForest(medv ~., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat_rf <- predict(rf_boston, newdata = Boston[-train, ])
rf_boston
mean((yhat_rf - boston_test)^2)
```
The test set is 11.31;; this indicates that random forests yielded an improvement over bagging in this case.
Using the importance() function, we can view the importance of each variable.
```{r}
importance(rf_boston)
```
Two measures of variable importance are reported. 

%IncMS is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. 

IncNodePurity is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. 
In the case of regression trees, the node impurity is measured by the training RSS, and for classifciation trees by the deviance. Plots of these importance measures can be produced using the varImpPlot() function.
```{r}
varImpPlot(rf_boston)
```
The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.

# Boosting

Here we use the gbm package, and with the gbm() function, to fit boosted regression trees to the Boston data set. We run gbm() with the option distribution="gaussian" since this is a regression problem; if it were a binary classification problem, we would use distribution = "bernoulli". The argument n.trees = 5000 indicates that we want 5000 trees, and the option interaction.depth=4 limit the depth of each tree.
```{r}
library(gbm)
set.seed(1)

boost_boston <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
boost_boston
summary(boost_boston)

```
The summary function produces a relative influence plot and also outputs the relative influence statistics.
We see that lstat and rm are by far the most important variables.


We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.
```{r}
par(mfrow = c(1,2))
plot(boost_boston, i = "rm")
plot(boost_boston, i = "lstat")
```
We now use the boosted model to predict medv on the test set:
```{r}
yhat_boost <- predict(boost_boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat_boost - boston_test)^2)
```
The test MSE is similar to the test MSE for random forests and superior to that for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter lambda. The default value is 0.001, but this is easily modified: here we take lambda = 0.2
```{r}
boost_boston <- gbm(medv ~ ., data = Boston[train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2)
yhat_boost <- predict(boost_boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat_boost - boston_test)^2)
```
In this case, using lambda = 0.2 leads to a higher test MSE



